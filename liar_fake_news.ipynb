{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LIAR Dataset: Fake News Classification\n",
        "\n",
        "**6-Class Classification using TF-IDF + Logistic Regression**\n",
        "\n",
        "This notebook implements a simple fake news classifier using:\n",
        "- TF-IDF for feature extraction\n",
        "- Logistic Regression for classification\n",
        "- 6 classes: pants-fire, false, barely-true, half-true, mostly-true, true\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports & Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# Define label order mapping (from most false to most true)\n",
        "# This maps string labels to integers for classification\n",
        "LABEL_MAPPING = {\n",
        "    'pants-fire': 0,\n",
        "    'false': 1,\n",
        "    'barely-true': 2,\n",
        "    'half-true': 3,\n",
        "    'mostly-true': 4,\n",
        "    'true': 5\n",
        "}\n",
        "\n",
        "# Reverse mapping for displaying results\n",
        "LABEL_NAMES = {v: k for k, v in LABEL_MAPPING.items()}\n",
        "\n",
        "print(\"✓ Imports and setup complete\")\n",
        "print(f\"Label mapping: {LABEL_MAPPING}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Data\n",
        "\n",
        "**What happens here:**\n",
        "- We read the TSV files from the `Dataset/` folder\n",
        "- The files don't have headers, so we use `header=None`\n",
        "- We assign column names based on the dataset documentation\n",
        "- We extract only the label (column 2) and statement (column 3) columns\n",
        "- We handle any missing values in the statement column\n",
        "\n",
        "**Note:** If your data is in a `data/` folder instead, change `Dataset/` to `data/` in the file paths below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define column names based on LIAR dataset documentation\n",
        "# Column 1: ID, Column 2: Label, Column 3: Statement, ...\n",
        "column_names = [\n",
        "    'id', 'label', 'statement', 'subject', 'speaker', 'job_title',\n",
        "    'state', 'party', 'barely_true_counts', 'false_counts',\n",
        "    'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'\n",
        "]\n",
        "\n",
        "# Load the datasets\n",
        "# Note: Update the path if your data is in a different folder (e.g., 'data/' instead of 'Dataset/')\n",
        "print(\"Loading datasets...\")\n",
        "train_df = pd.read_csv('Dataset/train.tsv', sep='\\t', header=None, names=column_names)\n",
        "valid_df = pd.read_csv('Dataset/valid.tsv', sep='\\t', header=None, names=column_names)\n",
        "test_df = pd.read_csv('Dataset/test.tsv', sep='\\t', header=None, names=column_names)\n",
        "\n",
        "print(f\"\\nDataset shapes:\")\n",
        "print(f\"Train: {train_df.shape}\")\n",
        "print(f\"Valid: {valid_df.shape}\")\n",
        "print(f\"Test: {test_df.shape}\")\n",
        "\n",
        "# Select only label and statement columns (ignore other metadata)\n",
        "train_df = train_df[['label', 'statement']].copy()\n",
        "valid_df = valid_df[['label', 'statement']].copy()\n",
        "test_df = test_df[['label', 'statement']].copy()\n",
        "\n",
        "# Handle missing statements (drop rows with NaN statements)\n",
        "print(f\"\\nMissing statements:\")\n",
        "print(f\"Train: {train_df['statement'].isna().sum()}\")\n",
        "print(f\"Valid: {valid_df['statement'].isna().sum()}\")\n",
        "print(f\"Test: {test_df['statement'].isna().sum()}\")\n",
        "\n",
        "train_df = train_df.dropna(subset=['statement'])\n",
        "valid_df = valid_df.dropna(subset=['statement'])\n",
        "test_df = test_df.dropna(subset=['statement'])\n",
        "\n",
        "print(f\"\\nAfter dropping missing statements:\")\n",
        "print(f\"Train: {train_df.shape}\")\n",
        "print(f\"Valid: {valid_df.shape}\")\n",
        "print(f\"Test: {test_df.shape}\")\n",
        "\n",
        "# Show label distribution\n",
        "print(f\"\\nLabel distribution in training set:\")\n",
        "print(train_df['label'].value_counts().sort_index())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Preprocess\n",
        "\n",
        "**What happens here:**\n",
        "- We clean the statement text (strip whitespace, convert to lowercase)\n",
        "- We encode string labels into integers using our mapping\n",
        "- This prepares the data for machine learning algorithms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"Basic text cleaning: strip and lowercase\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    return str(text).strip().lower()\n",
        "\n",
        "def encode_labels(df, label_mapping):\n",
        "    \"\"\"Encode string labels to integers\"\"\"\n",
        "    df = df.copy()\n",
        "    df['label_encoded'] = df['label'].map(label_mapping)\n",
        "    # Drop rows with unmapped labels (if any)\n",
        "    df = df.dropna(subset=['label_encoded'])\n",
        "    return df\n",
        "\n",
        "# Clean statement text\n",
        "print(\"Preprocessing statements...\")\n",
        "train_df['statement'] = train_df['statement'].apply(preprocess_text)\n",
        "valid_df['statement'] = valid_df['statement'].apply(preprocess_text)\n",
        "test_df['statement'] = test_df['statement'].apply(preprocess_text)\n",
        "\n",
        "# Encode labels to integers\n",
        "print(\"Encoding labels...\")\n",
        "train_df = encode_labels(train_df, LABEL_MAPPING)\n",
        "valid_df = encode_labels(valid_df, LABEL_MAPPING)\n",
        "test_df = encode_labels(test_df, LABEL_MAPPING)\n",
        "\n",
        "# Extract X (features) and y (labels) for each split\n",
        "X_train = train_df['statement'].values\n",
        "y_train = train_df['label_encoded'].values\n",
        "\n",
        "X_valid = valid_df['statement'].values\n",
        "y_valid = valid_df['label_encoded'].values\n",
        "\n",
        "X_test = test_df['statement'].values\n",
        "y_test = test_df['label_encoded'].values\n",
        "\n",
        "print(f\"\\nPreprocessing complete!\")\n",
        "print(f\"Train: {len(X_train)} samples\")\n",
        "print(f\"Valid: {len(X_valid)} samples\")\n",
        "print(f\"Test: {len(X_test)} samples\")\n",
        "print(f\"\\nSample statement: {X_train[0][:100]}...\")\n",
        "print(f\"Sample label: {y_train[0]} ({LABEL_NAMES[y_train[0]]})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Feature Extraction (TF-IDF)\n",
        "\n",
        "**What happens here:**\n",
        "- TF-IDF (Term Frequency-Inverse Document Frequency) converts text into numerical vectors\n",
        "- **CRITICAL:** We fit the vectorizer ONLY on training data to avoid data leakage\n",
        "- Data leakage would occur if we fit on all data (train+valid+test), giving the model information about validation/test sets\n",
        "- After fitting on train, we transform train/valid/test separately\n",
        "- Each statement becomes a vector of TF-IDF scores for each word in the vocabulary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize TF-IDF vectorizer\n",
        "# max_features limits vocabulary size (use top N most frequent words)\n",
        "# This helps reduce dimensionality and training time\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=10000,  # Use top 10,000 words\n",
        "    ngram_range=(1, 2),  # Use unigrams and bigrams (single words and word pairs)\n",
        "    min_df=2,  # Ignore words that appear in fewer than 2 documents\n",
        "    max_df=0.95  # Ignore words that appear in more than 95% of documents (stopwords)\n",
        ")\n",
        "\n",
        "print(\"Fitting TF-IDF vectorizer on TRAINING data only...\")\n",
        "print(\"(This is important to avoid data leakage!)\")\n",
        "\n",
        "# Fit ONLY on training data\n",
        "X_train_vectors = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform validation and test sets (using the vocabulary learned from train)\n",
        "X_valid_vectors = vectorizer.transform(X_valid)\n",
        "X_test_vectors = vectorizer.transform(X_test)\n",
        "\n",
        "print(f\"\\nFeature extraction complete!\")\n",
        "print(f\"Train vectors shape: {X_train_vectors.shape}\")\n",
        "print(f\"Valid vectors shape: {X_valid_vectors.shape}\")\n",
        "print(f\"Test vectors shape: {X_test_vectors.shape}\")\n",
        "print(f\"\\nVocabulary size: {len(vectorizer.vocabulary_)} words\")\n",
        "print(f\"\\nEach statement is now represented as a {X_train_vectors.shape[1]}-dimensional vector\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train Model\n",
        "\n",
        "**What happens here:**\n",
        "- We train a Logistic Regression classifier on the training vectors\n",
        "- We use the validation set to tune the hyperparameter C (regularization strength)\n",
        "- We try multiple C values and choose the one with the best macro-F1 score on validation set\n",
        "- Macro-F1 is the average F1 score across all classes (good for imbalanced datasets)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter tuning: Try different values of C (regularization strength)\n",
        "# Lower C = stronger regularization (prevents overfitting)\n",
        "# Higher C = weaker regularization (model can fit training data more closely)\n",
        "C_values = [0.1, 1.0, 10.0, 100.0, 1000.0]\n",
        "\n",
        "print(\"Tuning hyperparameter C on validation set...\")\n",
        "print(\"C values to try:\", C_values)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "best_score = -1\n",
        "best_C = None\n",
        "best_model = None\n",
        "\n",
        "for C in C_values:\n",
        "    # Train model with current C value\n",
        "    model = LogisticRegression(\n",
        "        C=C,\n",
        "        max_iter=1000,  # Maximum iterations for convergence\n",
        "        random_state=RANDOM_SEED,\n",
        "        multi_class='multinomial',  # For multi-class classification\n",
        "        solver='lbfgs'  # Good solver for multinomial logistic regression\n",
        "    )\n",
        "    \n",
        "    model.fit(X_train_vectors, y_train)\n",
        "    \n",
        "    # Evaluate on validation set\n",
        "    y_valid_pred = model.predict(X_valid_vectors)\n",
        "    macro_f1 = f1_score(y_valid, y_valid_pred, average='macro')\n",
        "    accuracy = accuracy_score(y_valid, y_valid_pred)\n",
        "    \n",
        "    print(f\"C = {C:6.1f} | Validation Accuracy: {accuracy:.4f} | Validation Macro-F1: {macro_f1:.4f}\")\n",
        "    \n",
        "    # Keep track of best model\n",
        "    if macro_f1 > best_score:\n",
        "        best_score = macro_f1\n",
        "        best_C = C\n",
        "        best_model = model\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(f\"\\n✓ Best C value: {best_C}\")\n",
        "print(f\"✓ Best validation macro-F1: {best_score:.4f}\")\n",
        "print(f\"\\nFinal model trained with C = {best_C}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluate on Test Set\n",
        "\n",
        "**What happens here:**\n",
        "- We evaluate the best model (chosen via validation) on the test set\n",
        "- This is the FINAL evaluation - we only touch the test set once\n",
        "- We compute accuracy, macro-F1, detailed classification report, and confusion matrix\n",
        "- The confusion matrix shows which classes are confused with each other\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set (FINAL evaluation - only done once!)\n",
        "print(\"Evaluating best model on TEST set...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "y_test_pred = best_model.predict(X_test_vectors)\n",
        "\n",
        "# Calculate metrics\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "test_macro_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
        "test_weighted_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
        "\n",
        "print(f\"\\nTest Set Results:\")\n",
        "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Macro-F1: {test_macro_f1:.4f}\")\n",
        "print(f\"Weighted-F1: {test_weighted_f1:.4f}\")\n",
        "\n",
        "# Classification report (precision, recall, F1 for each class)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Classification Report:\")\n",
        "print(\"=\"*60)\n",
        "target_names = [LABEL_NAMES[i] for i in sorted(LABEL_NAMES.keys())]\n",
        "print(classification_report(y_test, y_test_pred, target_names=target_names))\n",
        "\n",
        "# Confusion matrix\n",
        "print(\"=\"*60)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(\"=\"*60)\n",
        "cm = confusion_matrix(y_test, y_test_pred)\n",
        "\n",
        "# Display confusion matrix with labels\n",
        "cm_df = pd.DataFrame(\n",
        "    cm,\n",
        "    index=[f\"True {LABEL_NAMES[i]}\" for i in sorted(LABEL_NAMES.keys())],\n",
        "    columns=[f\"Pred {LABEL_NAMES[i]}\" for i in sorted(LABEL_NAMES.keys())]\n",
        ")\n",
        "print(cm_df)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Confusion Matrix Interpretation:\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nThe confusion matrix shows how predictions compare to true labels.\")\n",
        "print(\"Diagonal values = correct predictions\")\n",
        "print(\"Off-diagonal values = misclassifications\")\n",
        "print(\"\\nCommon pattern: Adjacent labels (e.g., 'false' vs 'barely-true') are often confused,\")\n",
        "print(\"which makes sense since they represent similar levels of truthfulness.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save Artifacts (Optional)\n",
        "\n",
        "**What happens here:**\n",
        "- We save the trained TF-IDF vectorizer and model using joblib\n",
        "- This allows us to reuse the model later without retraining\n",
        "- The vectorizer must be saved too, since we need it to transform new text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the vectorizer and model\n",
        "print(\"Saving artifacts...\")\n",
        "\n",
        "joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')\n",
        "joblib.dump(best_model, 'logistic_regression_model.joblib')\n",
        "\n",
        "print(\"✓ Saved: tfidf_vectorizer.joblib\")\n",
        "print(\"✓ Saved: logistic_regression_model.joblib\")\n",
        "\n",
        "# Example: How to load and use the saved model\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Example: Loading and using saved model:\")\n",
        "print(\"=\"*60)\n",
        "print(\"\"\"\n",
        "# Load artifacts\n",
        "vectorizer = joblib.load('tfidf_vectorizer.joblib')\n",
        "model = joblib.load('logistic_regression_model.joblib')\n",
        "\n",
        "# Predict on new text\n",
        "new_text = \"Your statement here\"\n",
        "new_text_cleaned = preprocess_text(new_text)\n",
        "new_vector = vectorizer.transform([new_text_cleaned])\n",
        "prediction = model.predict(new_vector)[0]\n",
        "predicted_label = LABEL_NAMES[prediction]\n",
        "print(f\"Predicted label: {predicted_label}\")\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Execution Flow Summary\n",
        "\n",
        "This section explains the complete pipeline as a sequence of data transformations:\n",
        "\n",
        "## Pipeline Overview\n",
        "\n",
        "```\n",
        "Raw TSV Files → Loaded DataFrames → Preprocessed Text → TF-IDF Vectors → Trained Model → Predictions\n",
        "```\n",
        "\n",
        "## Step-by-Step Flow\n",
        "\n",
        "### 1. **Data Loading**\n",
        "   - **Input:** TSV files (train.tsv, valid.tsv, test.tsv)\n",
        "   - **Process:** Read files without headers, assign column names, extract label + statement columns\n",
        "   - **Output:** Three DataFrames (train_df, valid_df, test_df) with 'label' and 'statement' columns\n",
        "\n",
        "### 2. **Text Preprocessing**\n",
        "   - **Input:** Raw statement text strings\n",
        "   - **Process:** Strip whitespace, convert to lowercase\n",
        "   - **Output:** Cleaned text strings\n",
        "\n",
        "### 3. **Label Encoding**\n",
        "   - **Input:** String labels ('pants-fire', 'false', etc.)\n",
        "   - **Process:** Map strings to integers (0-5) using LABEL_MAPPING\n",
        "   - **Output:** Integer labels (y_train, y_valid, y_test)\n",
        "\n",
        "### 4. **Feature Extraction (TF-IDF)**\n",
        "   - **Input:** Cleaned text strings (X_train, X_valid, X_test)\n",
        "   - **Process:** \n",
        "     - **Fit** vectorizer on X_train only (learns vocabulary)\n",
        "     - **Transform** X_train, X_valid, X_test into sparse matrices\n",
        "   - **Output:** Numerical vectors (X_train_vectors, X_valid_vectors, X_test_vectors)\n",
        "   - **Why fit only on train?** Prevents data leakage - the model shouldn't see validation/test vocabulary during training\n",
        "\n",
        "### 5. **Model Training & Validation**\n",
        "   - **Input:** Training vectors (X_train_vectors, y_train)\n",
        "   - **Process:** \n",
        "     - Train Logistic Regression models with different C values\n",
        "     - Evaluate each on validation set (X_valid_vectors, y_valid)\n",
        "     - Select best C based on macro-F1 score\n",
        "   - **Output:** Best trained model (best_model)\n",
        "\n",
        "### 6. **Final Evaluation**\n",
        "   - **Input:** Test vectors (X_test_vectors, y_test) and best_model\n",
        "   - **Process:** \n",
        "     - Make predictions on test set\n",
        "     - Calculate accuracy, macro-F1, classification report, confusion matrix\n",
        "   - **Output:** Performance metrics and predictions\n",
        "   - **Important:** Test set is only evaluated once - this is the final, unbiased estimate of model performance\n",
        "\n",
        "### 7. **Model Persistence**\n",
        "   - **Input:** Trained vectorizer and model\n",
        "   - **Process:** Save to disk using joblib\n",
        "   - **Output:** Saved files (tfidf_vectorizer.joblib, logistic_regression_model.joblib)\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "### Data Leakage Prevention\n",
        "- **Problem:** If we fit TF-IDF on all data (train+valid+test), the model gets information about validation/test sets\n",
        "- **Solution:** Fit vectorizer only on training data, then transform all sets\n",
        "- **Result:** Model only learns from training data, ensuring fair evaluation\n",
        "\n",
        "### Train/Validation/Test Split Purpose\n",
        "- **Training set:** Used to learn model parameters and vocabulary\n",
        "- **Validation set:** Used to tune hyperparameters (C) and select best model\n",
        "- **Test set:** Used for final, unbiased evaluation (touched only once)\n",
        "\n",
        "### TF-IDF Vectorization\n",
        "- Converts text documents into numerical feature vectors\n",
        "- Each word gets a TF-IDF score based on:\n",
        "  - **TF (Term Frequency):** How often the word appears in the document\n",
        "  - **IDF (Inverse Document Frequency):** How rare/common the word is across all documents\n",
        "- Words that appear in many documents (like \"the\", \"is\") get lower IDF scores\n",
        "- Words unique to specific documents get higher TF-IDF scores\n",
        "\n",
        "### Logistic Regression\n",
        "- A linear classifier that learns weights for each feature (word)\n",
        "- For each class, it computes a score based on weighted sum of TF-IDF features\n",
        "- The class with highest score is predicted\n",
        "- C parameter controls regularization (prevents overfitting)\n",
        "\n",
        "## Data Flow Diagram\n",
        "\n",
        "```\n",
        "train.tsv ──┐\n",
        "            ├──> Load & Extract ──> Preprocess ──> Fit TF-IDF ──> Train Model ──> Best Model\n",
        "valid.tsv ──┤                                                      │\n",
        "            │                                                      │\n",
        "test.tsv ───┘                                                      │\n",
        "            │                                                      │\n",
        "            └──> Load & Extract ──> Preprocess ──> Transform ──> Evaluate ──> Metrics\n",
        "```\n",
        "\n",
        "This pipeline ensures that:\n",
        "1. No information from validation/test sets leaks into training\n",
        "2. Hyperparameters are tuned on validation set (not test set)\n",
        "3. Test set provides unbiased final evaluation\n",
        "4. The model can be saved and reused for new predictions\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
